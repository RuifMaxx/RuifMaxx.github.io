---
layout: post
title: Pretation in 12.14
categories: Pretation
description: 
keywords: time series
---



# VRNN

## RNN background

### Shortcoming of RNN

**The only source of randomness or variability in the RNN is found in the conditional output probability model. **

We suggest that this can be an inappropriate way to model the kind of variability observed in highly structured data, such as natural speech, which is characterized by strong and complex dependencies among the output variables at different timesteps. 



### Structure of RNN

 updates its hidden state *ht* by:
 
 <br/>
 
 <img src="RuifMaxx.github.io\images\12.14\21.png" style="zoom: 67%;" />

 <br/>
 

where *f* is a deterministic non-linear transition function, and $\theta$ is the parameter set of *f*. The transition function *f* can be implemented with gated activation functions such as long short-term memory [LSTM, 9] or gated recurrent unit [GRU, 5]. RNNs model sequences by parameterizing a factorization of the joint sequence probability distribution as a product of conditional probabilities such that:

$$
\begin{aligned}
p\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{T}\right) &=\prod_{t=1}^{T} p\left(\mathbf{x}_{t} \mid \mathbf{x}_{<t}\right) \\
p\left(\mathbf{x}_{t} \mid \mathbf{x}_{<t}\right) &=g_{\tau}\left(\mathbf{h}_{t-1}\right)
\end{aligned}
$$

where *g* is a function that maps the RNN hidden state *ht-1* to a probability distribution over possible outputs, and $\tau$ is the parameter set of *g*.

![img](https://RuifMaxx.github.io/images/12.14/1.jpg)



With a deterministic transition function f, the choice of g effectively defines the family of joint probability distributions p(x1, . . . , xT ) that can be expressed by the RNN 



## GMM

**高斯混合模型是对高斯模型进行简单的扩展，GMM使用多个高斯分布的组合来刻画数据分布。**



举例来说：想象下现在咱们不再考察全部用户的身高，而是要在模型中同时考虑男性和女性的身高。假定之前的样本里男女都有，那么之前所画的高斯分布其实是两个高斯分布的叠加的结果。相比只使用一个高斯来建模，现在我们可以用两个（或多个）高斯分布（陈运文）：
$$
p(x)=\sum_{i=1}^{K} \phi_{i} \frac{1}{\sqrt{2 \sigma_{i}^{2} \pi}} e^{-\frac{\left(x-\mu_{i}\right)^{2}}{2 \sigma_{i}^{2}}}
$$


**该公式和之前的公式非常相似，细节上有几点差异。**首先分布概率是K个高斯分布的和，每个高斯分布有属于自己的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 参数，以及对应的权重参数，权重值必须为正数，所有权重的和必须等于1，以确保公式给出数值是合理的概率密度值。换句话说如果我们把该公式对应的输入空间合并起来，结果将等于1。



回到之前的例子，女性在身高分布上通常要比男性矮，画成图的话如图3

![img](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\2.jpg)

(https://zhuanlan.zhihu.com/p/31103654)
$$
\begin{aligned}
p\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{T}\right) &=\prod_{t=1}^{T} p\left(\mathbf{x}_{t} \mid \mathbf{x}_{<t}\right) \\
p\left(\mathbf{x}_{t} \mid \mathbf{x}_{<t}\right) &=g_{\tau}\left(\mathbf{h}_{t-1}\right)
\end{aligned}
$$
![image-20201213162908196](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\5.png)
$$
p_{\boldsymbol{\alpha}_{t}, \boldsymbol{\mu}_{,, t}, \Sigma_{\cdot, t}}\left(\mathbf{x}_{t} \mid \mathbf{x}_{<t}\right)=\sum_{j} \alpha_{j, t} \mathcal{N}\left(\mathbf{x}_{t} ; \boldsymbol{\mu}_{j, t}, \Sigma_{j, t}\right)
$$

(https://arxiv.org/abs/1506.02216)



## VAE background

首先我们有一批数据样本{X1,…,Xn}，其整体用X来描述，我们本想根据{X1,…,Xn}得到X的分布p(X)，如果能得到的话，那我直接根据p(X)来采样，就可以得到所有可能的X了（包括{X1,…,Xn}以外的），这是一个终极理想的生成模型了。当然，这个理想很难实现，于是我们将分布改一改

p(X)=∑ p(X|Z)p(Z)(1)

这里我们就不区分求和还是求积分了，意思对了就行。此时p(X|Z)就描述了一个由Z来生成X的模型，而我们假设Z服从标准正态分布，也就是p(Z)=N(0,I)

([变分自编码器（一）：原来是这么一回事 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/5253))

![事实上，vae是为每个样本构造专属的正态分布，然后采样来重构](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\3.png)

$$
\begin{array}{rlr}
\log p(x) & =  \log \int p(x \mid z) p(z) d z \\
& =\log \int \frac{p(x \mid z) p(z)}{q(z \mid x)} q(z \mid x) d z \\
& =\log \mathbb{E}_{q(z \mid x)}\left[\frac{p(x \mid z) p(z)}{q(z \mid x)}\right]
\end{array}
$$
根据 Jenson 不等式
$$
\log p(x) \geq \mathbb{E}_{q(z \mid x)} \log \left[\frac{p(x \mid z) p(z)}{q(z \mid x)}\right]
$$


得到变分下界：
$$
E_{q(z \mid x)} \log p(x \mid z)-K L(q(z \mid x) \| p(z))
$$


其中第一项用可用对数正态分布表示为：

$$
\log \left(\frac{1}{\sqrt{2 \pi} \sigma} e^{\left.\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)}\right.
$$
写成代码是：



```python
def log_normal_pdf(x, mean, logvar):
    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device)
    const = torch.log(const)
    return -.5 * ( + logvar + (x - mean) ** 2. / torch.exp(logvar))

logpx = log_normal_pdf(
                samp_trajs, pred_x, noise_logvar).sum(-1).sum(-1)
```



**当X满足伯努利分布时：**

 由极大似然估计的定义可知：
$$
\begin{aligned}
&\text { }\\
&\theta=\underset{\theta}{\operatorname{argmax}} p(X \mid \theta)\\
&=\underset{\theta}{\operatorname{argmax}} \prod_{i=1}^{N} p\left(x^{(i)} \mid \theta\right)\\
&=\underset{\theta}{\operatorname{argmax}} \frac{1}{N} \prod_{i=1}^{N} p\left(x^{(i)} \mid \theta\right)\\
&=\underset{\theta}{\operatorname{argmax}} \frac{1}{N} \sum_{i=1}^{N} \log p\left(x^{(i)} \mid \theta\right)
\end{aligned}
$$
由于样本X由
$$
p_{\text {data}}(x)
$$
采集而来，因此等同于：
$$
=\underset{\theta}{\operatorname{argmax}} E_{x \sim p_{\text {data}}(x)} \log p(x \mid \theta)
$$

$$
\begin{array}{l}
=\underset{\theta}{\operatorname{argmax}} \int_{x} p_{\text {data}}(x) \log p(x \mid \theta) d x \\
=\underset{\theta}{\operatorname{argmin}} \int_{x}-p_{\text {data}}(x) \log p(x \mid \theta) d x
\end{array}
$$
上式就是交叉嫡损失，所以极大似然与最小化交叉嫡损失此时等价。

([极大似然估计与最小化交叉熵损失或者KL散度为什么等价？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/84764177))

下图为keras的代码

(https://keras.io/examples/generative/vae/)

![image-20201213165046766](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\4.png)

## VRNN

Unlike a standard VAE, the prior on the latent random variable is no longer a standard Gaussian distribution, but follows the distribution:

Z的先验分布：
$$
p\left(\mathbf{z}_{t} \mid \mathbf{x}_{<t}, \mathbf{z}_{<t}\right)
$$

$$
\mathbf{z}_{t} \sim \mathcal{N}\left(\boldsymbol{\mu}_{0, t}, \operatorname{diag}\left(\boldsymbol{\sigma}_{0, t}^{2}\right)\right), \text { where }\left[\boldsymbol{\mu}_{0, t}, \boldsymbol{\sigma}_{0, t}\right]=\varphi_{\tau}^{\text {prior }}\left(\mathbf{h}_{t-1}\right)
$$

生成量X的分布：
$$
p\left(\mathbf{x}_{t} \mid \mathbf{z}_{\leq t}, \mathbf{x}_{<t}\right)
$$

$$
\mathbf{x}_{t} \mid \mathbf{z}_{t} \sim \mathcal{N}\left(\boldsymbol{\mu}_{x, t}, \operatorname{diag}\left(\boldsymbol{\sigma}_{x, t}^{2}\right)\right), \text { where }\left[\boldsymbol{\mu}_{x, t}, \boldsymbol{\sigma}_{x, t}\right]=\varphi_{\tau}^{\operatorname{dec}}\left(\varphi_{\tau}^{\mathbf{z}}\left(\mathbf{z}_{t}\right), \mathbf{h}_{t-1}\right)
$$

RNN更新：
$$
\mathbf{h}_{t}=f_{\theta}\left(\varphi_{\tau}^{\mathbf{x}}\left(\mathbf{x}_{t}\right), \varphi_{\tau}^{\mathbf{z}}\left(\mathbf{z}_{t}\right), \mathbf{h}_{t-1}\right)
$$
![image-20201213165709610](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\6.png)

Z的后验分布：
$$
q\left(\mathbf{z}_{\leq T} \mid \mathbf{x}_{\leq T}\right)=\prod_{t=1}^{T} q\left(\mathbf{z}_{t} \mid \mathbf{x}_{\leq t}, \mathbf{z}_{<t}\right)
$$

$$
\mathbf{z}_{t} \mid \mathbf{x}_{t} \sim \mathcal{N}\left(\boldsymbol{\mu}_{z, t}, \operatorname{diag}\left(\boldsymbol{\sigma}_{z, t}^{2}\right)\right), \text { where }\left[\boldsymbol{\mu}_{z, t}, \boldsymbol{\sigma}_{z, t}\right]=\varphi_{\tau}^{\mathrm{enc}}\left(\varphi_{\tau}^{\mathbf{x}}\left(\mathbf{x}_{t}\right), \mathbf{h}_{t-1}\right)
$$

变分下界：
$$
\mathbb{E}_{q(\mathbf{z} \leq T \mid \mathbf{x} \leq T)}\left[\sum_{t=1}^{T}\left(-\operatorname{KL}\left(q\left(\mathbf{z}_{t} \mid \mathbf{x}_{\leq t}, \mathbf{z}_{<t}\right) \| p\left(\mathbf{z}_{t} \mid \mathbf{x}_{<t}, \mathbf{z}_{<t}\right)\right)+\log p\left(\mathbf{x}_{t} \mid \mathbf{z}_{\leq t}, \mathbf{x}_{<t}\right)\right)\right]
$$
优点：为RNN带来随机性，利用生成模型隐变量Z很好地反映X



![image-20201213171115516](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\7.png)

![image-20201213171134049](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\8.png)

# Neural ode 

![image-20201213171420543](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\9.png)

![image-20201213171553223](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\10.png)

原本库里面是二维的，issues里面有一维的版本

(https://github.com/rtqichen/torchdiffeq/issues/91)

[^]: 所以我花了一些时间来研究代码并找到了主要问题。在修改代码时，您可能已经注意到了这一点。我最初能够对所有轨迹使用相同的样本的主要原因是目标是在（x，y）坐标的2D空间中生成数据。真正重要的是观察之间的时间间隔，时间戳的精确值并不重要。 当目标是准确地将Latent ODE的动态与基本真相的动态一致时，问题就变得稍微复杂了。当然，在这种情况下，您不应该假设批处理中的所有数据项都是从同一开始时间生成的，因为实际上这并不是它的工作方式。因此，更合适的做法是正确地提取所有时间戳，并根据查询到的时间戳对每个条目进行集成。您可能还希望将Latent ODE参数化为时间不均匀。 但是我认为库目前不支持集成批处理时间，因此如果您的数据包含具有不同时间戳的不同条目，最好的办法是将这些条目进行并集，将批中的所有条目集成到联合中的所有时间，然后根据各自的时间戳选择正确的值。 尤利娅的项目在这方面取得了一些进展，她的代码是开源的。

![image-20201213175457407](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\12.png)

# Latent ODE



![image-20201213173246240](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\13.png)



![image-20201213173311255](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\14.png)

![image-20201213173658600](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\15.png)

![image-20201213175605871](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\16.png)

the joint distribution is defined as:
$$
p(\mathbf{x}, \mathbf{t})=p(\mathbf{x} \mid \mathbf{t}) p(\mathbf{t})
$$


![image-20201213175704761](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\17.png)

# Tricks



## latent_ode

发现原程序的`samp_trajs`和[Pull Request](https://github.com/rtqichen/torchdiffeq/pull/5)里面的这张[图片](https://user-images.githubusercontent.com/12689993/49614423-3a0b2200-f978-11e8-89da-5dcdf5cfd240.png)很不一样，就做了修改。



![image-20201213180119680](C:\Users\hp\AppData\Roaming\Typora\typora-user-images\image-20201213180119680.png)

![image-20201213172534789](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\11.png)

## ode-demo参数修改



![image-20201213180229260](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\18.png)

![image-20201213180318354](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\19.png)



1.扩大batch_size： 发现训练集上面的loss已经降得很小而测试集的loss乱跳，这是由于训练集过小导致的。 尝试扩大batch_time和batch_size，发现单独扩大batch_size效果较好。

2.扩大神经网络的宽度： 发现随着积分次数的增加模型拟合能力变弱，这是神经网络复杂度不够导致的。 尝试扩大神经网络的层数或宽度，发现扩大宽度效果较好。

3.更换优化器，添加正则项： 把优化器从rmsprop换成Adam后精度有明显提高，同时运行速度变快。 进一步换成带正则项的AdamW，精度进一步提高。

4.动态调整学习率： 开始时快速拟合曲线形状，之后微调幅度。

5.早停法： 当模型性能不再提高时，停止训练。

![image-20201213180336539](E:\Documents\GitHub\RuifMaxx.github.io\images\12.14\20.png)

